"""
å¿«é€ŸåŒ»å­¦æ–‡çŒ®æ‘˜è¦æœç´¢å·¥å…·ï¼Œæ•´åˆå¤šæºä¿¡æ¯æ£€ç´¢

æ­¤å·¥å…·åˆ©ç”¨LangChainçš„å›è°ƒæœºåˆ¶å®æ—¶åé¦ˆç ”ç©¶è¿›åº¦
"""


import re
import asyncio
import logging

from typing import List, Optional
from urllib.parse import quote_plus
import httpx

from googletrans import Translator
from langchain_core.tools import BaseTool
from bs4 import BeautifulSoup
from pydantic import BaseModel, Field
from langchain_core.callbacks import CallbackManagerForToolRun, AsyncCallbackManagerForToolRun
import requests

from src.tools.pubmed_crawle import run_pubmed

from src.tools.decorators import create_logged_tool

BROWSER_HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',
    'Accept-Language': 'en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7',
    'Accept-Encoding': 'gzip, deflate, br', # httpx/requests ä¼šå¤„ç†è§£å‹
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1',
    'Sec-Fetch-Dest': 'document',
    'Sec-Fetch-Mode': 'navigate',
    'Sec-Fetch-Site': 'same-origin', # æˆ– 'none' å–å†³äºåœºæ™¯
    'Sec-Fetch-User': '?1',
}

# é…ç½®æ›´è¯¦ç»†çš„æ—¥å¿—è®°å½•
logger = logging.getLogger(__name__)

# ç¡®ä¿æ—¥å¿—çº§åˆ«æ˜¯DEBUGï¼Œä»¥æ˜¾ç¤ºæ‰€æœ‰è°ƒè¯•ä¿¡æ¯
logger.setLevel(logging.DEBUG)

# æ·»åŠ æ§åˆ¶å°å¤„ç†å™¨ï¼Œç¡®ä¿æ—¥å¿—ä¼šæ‰“å°åˆ°æ§åˆ¶å°
if not logger.handlers:
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.DEBUG)
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)
    
logger.debug("fast_abstract_toolæ¨¡å—å·²åŠ è½½")

class PubMedResult(BaseModel):
    """PubMedæœç´¢ç»“æœæ¨¡å‹"""
    pmid: str
    title: Optional[str] = None
    abstract: str
    url: str

class FastAbstractToolInput(BaseModel):
    """åŒ»å­¦æ–‡çŒ®å¿«é€Ÿæœç´¢å·¥å…·çš„è¾“å…¥æ¨¡å‹"""
    query: str = Field(..., description="æœç´¢å…³é”®è¯æˆ–åŒ»å­¦é—®é¢˜")
    num_results: int = Field(5, description="éœ€è¦çš„ç»“æœæ•°é‡")
    sources: List[str] = Field(default=["pubmed"], description="è¦æœç´¢çš„æ¥æºï¼Œå¯é€‰å€¼åŒ…æ‹¬ï¼špubmed, google_scholar, medlineç­‰")

class FastAbstractTool(BaseTool):
    name: str = "fast_abstract_tool"
    description: str = "..." # çœç•¥
    args_schema = FastAbstractToolInput

    # --- åŒæ­¥ _run æ–¹æ³• (å°è¯•ç”¨ asyncio.run è°ƒç”¨ _arun) ---
    def _run(
        self,
        query: str,
        num_results: int = 5,
        sources: List[str] = ["pubmed"],
        run_manager: Optional[CallbackManagerForToolRun] = None # æ¥æ”¶åŒæ­¥ Callback Manager
    ) -> str:
        """
        åŒæ­¥æ‰§è¡Œå…¥å£ã€‚å°è¯•åœ¨å½“å‰çº¿ç¨‹ä½¿ç”¨ asyncio.run() æ‰§è¡Œå¼‚æ­¥ç‰ˆæœ¬ _arunã€‚
        æ³¨æ„ï¼šè¿™å¯èƒ½ä¼šä¸¢å¤± _arun ä¸­çš„ä¸­é—´å›è°ƒè¿›åº¦ã€‚
        """
        logger.warning(f"Tool '{self.name}' called synchronously (_run). Attempting to execute asynchronous version (_arun) using asyncio.run(). Intermediate callbacks may be lost.")

        # æŠ¥å‘Šä¸€ä¸ªåˆå§‹çŠ¶æ€ï¼ˆå¯é€‰ï¼‰ï¼Œå› ä¸º _arun çš„å›è°ƒä¸ä¼šè¢«è§¦å‘
        if run_manager:
            run_manager.on_text("åŒæ­¥è°ƒç”¨ï¼Œå¼€å§‹æ‰§è¡Œå¼‚æ­¥æ ¸å¿ƒé€»è¾‘...\n", verbose=True)

        try:
            # asyncio.run() ä¼šåœ¨æ­¤çº¿ç¨‹ä¸­åˆ›å»ºã€è¿è¡Œå¹¶å…³é—­ä¸€ä¸ªæ–°çš„äº‹ä»¶å¾ªç¯
            result = asyncio.run(self._arun(
                query=query,
                num_results=num_results,
                sources=sources,
                run_manager=None # æ˜¾å¼å°† run_manager è®¾ç½®ä¸º None
            ))
            if run_manager:
                 run_manager.on_text("å¼‚æ­¥æ ¸å¿ƒé€»è¾‘æ‰§è¡Œå®Œæ¯•ã€‚\n", verbose=True)
            return result
        except Exception as e:
            logger.error(f"Failed to execute _arun using asyncio.run() from _run: {e}", exc_info=True)
            # å‘ LangChain æ¡†æ¶è¿”å›ä¸€ä¸ªæœ‰æ„ä¹‰çš„é”™è¯¯ä¿¡æ¯
            error_message = f"Error: Tool execution failed during synchronous fallback. Details: {e}"
            # ä¹Ÿå¯ä»¥å°è¯•é€šè¿‡ run_manager æŠ¥å‘Šé”™è¯¯ï¼Œå¦‚æœ run_manager å­˜åœ¨
            if run_manager:
                 run_manager.on_text(f"âŒ {error_message}\n", verbose=True)
            # è¿”å›é”™è¯¯ä¿¡æ¯æˆ–è€…æ ¹æ®éœ€è¦æŠ›å‡ºå¼‚å¸¸ï¼ˆä½†è¿™å¯èƒ½ä¸­æ–­æµç¨‹ï¼‰
            return error_message # è¿”å›é”™è¯¯ä¿¡æ¯å­—ç¬¦ä¸²é€šå¸¸æ›´å®‰å…¨


    # --- éœ€è¦åˆ›å»ºä¸€ä¸ªåŒæ­¥çš„ PubMed è·å–æ–¹æ³• ---
    def _fetch_pubmed_abstracts_sync(
        self,
        query: str,
        num_results: int = 5,
        max_page: int = 10,
        run_manager: Optional[CallbackManagerForToolRun] = None
    ) -> List[PubMedResult]:
        """(åŒæ­¥ç‰ˆæœ¬) è·å–PubMedæ–‡ç« æ‘˜è¦"""
        logger.debug(f"[è°ƒè¯•] _fetch_pubmed_abstracts_sync: query={query}, num_results={num_results}")
        encoded_query = quote_plus(query)
        results = []
        page = 1

        # ä½¿ç”¨ requests è¿›è¡ŒåŒæ­¥è¯·æ±‚
        with requests.Session() as session: # ä½¿ç”¨ Session ä¿æŒè¿æ¥
             session.headers.update({'User-Agent': '...'}) # è®¾ç½® User-Agent
             while len(results) < num_results and page <= max_page:
                if run_manager:
                    run_manager.on_text(f"æ­£åœ¨æœç´¢PubMedç¬¬{page}é¡µ...\n", verbose=True)

                url = f"https://pubmed.ncbi.nlm.nih.gov/?term={encoded_query}&page={page}"
                try:
                    response = session.get(url, timeout=20.0) # è®¾ç½®è¶…æ—¶
                    response.raise_for_status()
                    data = response.text

                    # ... (æå–é“¾æ¥ã€æ ‡é¢˜ã€æ‘˜è¦çš„é€»è¾‘åŸºæœ¬ä¸å˜) ...
                    pat1_content_url = '<div class="docsum-wrap">.*?<.*?href="(.*?)".*?</a>'
                    content_url = re.compile(pat1_content_url, re.S).findall(data)

                    if not content_url:
                        # ... (å¤„ç†æ— ç»“æœ) ...
                        break

                    for idx, article_url in enumerate(content_url):
                        if len(results) >= num_results: break
                        full_url = "https://pubmed.ncbi.nlm.nih.gov" + article_url

                        if run_manager and idx % 3 == 0:
                             # ... (æŠ¥å‘Šè¿›åº¦) ...
                             run_manager.on_text(f"æ­£åœ¨è·å–æ–‡çŒ® {len(results)+1}/{num_results}...\n", verbose=True)

                        try:
                            article_response = session.get(full_url, timeout=20.0)
                            article_response.raise_for_status()
                            article_data = article_response.text
                            # ... (æå– PMID, title, abstract) ...
                            # å‡è®¾æå–é€»è¾‘ä¸å˜
                            pmid = "..."
                            title = "..."
                            abstract = "..."
                            abstract_found = True

                            if abstract_found:
                                results.append(PubMedResult(pmid=pmid, title=title, abstract=abstract, url=full_url))
                                if run_manager:
                                     run_manager.on_text(f"âœ“ å·²è·å–: {title[:40]}...\n", verbose=True)

                        except requests.exceptions.RequestException as req_err:
                             logger.error(f"è·å–æ–‡ç« è¯¦æƒ…è¯·æ±‚é”™è¯¯ {full_url}: {req_err}")
                             if run_manager: run_manager.on_text(f"è·å–æ–‡ç«  {pmid} è¯¦æƒ…å¤±è´¥ (è¯·æ±‚é”™è¯¯)ï¼Œç»§ç»­\n", verbose=True)
                        except Exception as err:
                            logger.error(f"å¤„ç†PubMedæ–‡ç«  {full_url} æ—¶å‡ºç°é”™è¯¯: {err}", exc_info=True)
                            if run_manager: run_manager.on_text(f"è·å–æ–‡ç« è¯¦æƒ…å¤±è´¥ï¼Œç»§ç»­å°è¯•ä¸‹ä¸€ç¯‡\n", verbose=True)

                    page += 1

                except requests.exceptions.RequestException as req_err: # å¤„ç†åˆ—è¡¨é¡µè¯·æ±‚é”™è¯¯
                    logger.error(f"è¯·æ±‚PubMedç¬¬{page}é¡µæ—¶å‡ºé”™: {req_err}")
                    if run_manager: run_manager.on_text(f"æœç´¢PubMedç¬¬{page}é¡µè¯·æ±‚å‡ºé”™ï¼Œå°è¯•ä¸‹ä¸€é¡µ\n", verbose=True)
                    page += 1
                except Exception as err: # æ•è·å…¶ä»–æ„å¤–é”™è¯¯
                    logger.error(f"å¤„ç†PubMedç¬¬{page}é¡µæ—¶å‡ºç°æœªçŸ¥é”™è¯¯: {err}", exc_info=True)
                    if run_manager: run_manager.on_text(f"æœç´¢PubMedç¬¬{page}é¡µå‡ºé”™ï¼Œå°è¯•ä¸‹ä¸€é¡µ\n", verbose=True)
                    page += 1
        return results


    # --- å¼‚æ­¥ _arun æ–¹æ³• ---
    async def _arun(
        self,
        query: str,
        num_results: int = 5,
        sources: List[str] = ["pubmed"],
        run_manager: Optional[AsyncCallbackManagerForToolRun] = None # æ³¨æ„ç±»å‹æ˜¯ Async*
    ) -> str:
        """å¼‚æ­¥è¿è¡ŒåŒ»å­¦æ–‡çŒ®æœç´¢å·¥å…·"""
        logger.debug(f"[FastAbstractTool] _arun æ–¹æ³•è¢«è°ƒç”¨: æŸ¥è¯¢={query}, ç»“æœæ•°é‡={num_results}, æ¥æº={sources}")
        logger.debug(f"[FastAbstractTool] run_manager ç±»å‹: {type(run_manager)}")

        translator = Translator() # æ¯æ¬¡è°ƒç”¨åˆ›å»ºå®ä¾‹å¯èƒ½ä¸æ˜¯æœ€é«˜æ•ˆçš„ï¼Œå¯ä»¥è€ƒè™‘ç±»çº§åˆ«æˆ–å…¨å±€å®ä¾‹
        original_query = query
        translation_query = query # é»˜è®¤ä¸ç¿»è¯‘

        # æ£€æŸ¥æ˜¯å¦æœ‰ä¸­æ–‡å¹¶è¿›è¡Œç¿»è¯‘
        has_chinese = bool(re.search(r'[\u4e00-\u9fff]', query))
        if has_chinese:
            if run_manager:
                await run_manager.on_text(f"åŸå§‹æŸ¥è¯¢: '{query}'\n", verbose=True)
            try:
                logger.debug(f"å°è¯•ç¿»è¯‘æŸ¥è¯¢: '{query}'")
                # --- ä½¿ç”¨ await è°ƒç”¨å¼‚æ­¥ç¿»è¯‘ ---
                translation_query = translator.translate(query, dest='en', src='auto')
                # translation_query = translation_query.text

                logger.debug(f"æŸ¥è¯¢å·²ç¿»è¯‘ä¸ºè‹±æ–‡: '{translation_query}'")
                if run_manager:
                    await run_manager.on_text(f"æŸ¥è¯¢å·²ç¿»è¯‘ä¸ºè‹±æ–‡: '{translation_query}'\n", verbose=True)
            except Exception as e:
                logger.error(f"ç¿»è¯‘æŸ¥è¯¢æ—¶å‡ºé”™: {e}")
                if run_manager:
                    await run_manager.on_text(f"âŒ ç¿»è¯‘æŸ¥è¯¢å¤±è´¥: {e}\n", verbose=True)
                # å¯ä»¥é€‰æ‹©æ˜¯ç»§ç»­ä½¿ç”¨åŸå§‹æŸ¥è¯¢è¿˜æ˜¯è¿”å›é”™è¯¯
                # translated_query = query # å›é€€åˆ°åŸå§‹æŸ¥è¯¢

        # åç»­é€»è¾‘ä½¿ç”¨ translated_query
        query_to_use = translation_query

        all_results = []
        if run_manager:
            await run_manager.on_text("å¼€å§‹åŒ»å­¦æ–‡çŒ®æœç´¢...\n", verbose=True)

        if "pubmed" in sources:
            if run_manager:
                await run_manager.on_text(f"å¼€å§‹æ£€ç´¢PubMedåŒ»å­¦æ–‡çŒ®: '{query_to_use}'...\n", verbose=True)
            try:
                # --- è°ƒç”¨å¼‚æ­¥ç‰ˆæœ¬çš„ PubMed è·å–æ–¹æ³• ---
                pubmed_results = await asyncio.to_thread(run_pubmed, query_to_use, num_results)

                if pubmed_results:
                    all_results.extend(pubmed_results)
                    if run_manager:
                        await run_manager.on_text(f"âœ… å·²ä» PubMedè·å–{len(pubmed_results)}ç¯‡ç›¸å…³æ–‡çŒ®\n", verbose=True)
                else:
                     if run_manager:
                        await run_manager.on_text(f"âš ï¸ æœªä»PubMedæ‰¾åˆ°ä¸'{query_to_use}'ç›¸å…³çš„ç»“æœ\n", verbose=True)

            except Exception as e:
                error_msg = f"PubMedæœç´¢å‡ºé”™: {str(e)}"
                logger.error(error_msg, exc_info=True)
                if run_manager:
                    await run_manager.on_text(f"âŒ {error_msg}\n", verbose=True)

        # ... å¤„ç†å…¶ä»–æ¥æº ...

        # æ„å»ºæœ€ç»ˆç»“æœ (ä¸ _run ç±»ä¼¼ï¼Œä½†ä½¿ç”¨ await on_text)
        if not all_results:
            return f"æœªæ‰¾åˆ°ä¸'{original_query}' (è‹±æ–‡æŸ¥è¯¢: '{query_to_use}')ç›¸å…³çš„åŒ»å­¦æ–‡çŒ®æ‘˜è¦ã€‚"

        if run_manager:
            await run_manager.on_text(f"ğŸ“Š åŒ»å­¦æ–‡çŒ®æœç´¢å®Œæˆï¼Œå…±æ‰¾åˆ°{len(all_results)}ç¯‡ç›¸å…³æ–‡çŒ®\n", verbose=True)

        markdown = f"## åŒ»å­¦æ–‡çŒ®æœç´¢ç»“æœ\n\n"
        if has_chinese and original_query != query_to_use:
            markdown += f"**åŸå§‹æŸ¥è¯¢**: {original_query}\n"
            markdown += f"**è‹±æ–‡æŸ¥è¯¢**: {query_to_use}\n\n"
        else:
            markdown += f"**æŸ¥è¯¢**: {query_to_use}\n\n"
        
        markdown += f"**ç»“æœæ•°é‡**: {len(all_results)} ç¯‡\n\n"

        if len(all_results) > 0:
            markdown += "## æ–‡çŒ®åˆ—è¡¨\n\n"
            # --- > ä¿®æ­£ Markdown æ ¼å¼åŒ–: ä½¿ç”¨å­—å…¸é”®è®¿é—® < ---
            for i, result_dict in enumerate(all_results, 1): # é‡å‘½åå¾ªç¯å˜é‡ä¸º result_dict
                title = result_dict.get('title', 'N/A') # ä½¿ç”¨ .get() æ›´å®‰å…¨
                pmid = result_dict.get('pmid', 'N/A')
                url = result_dict.get('url', '#')
                abstract = result_dict.get('abstract', 'N/A')
                markdown += f"### {i}. {title}\n" # æ­£ç¡®è®¿é—® title
                markdown += f"**PMID**: {pmid} | [æŸ¥çœ‹åŸæ–‡]({url})\n\n"
                markdown += f"**æ‘˜è¦**:\n{abstract}\n\n"
                markdown += "---\n\n"

        return markdown

    # --- éœ€è¦å°† PubMed è·å–é€»è¾‘ä¹Ÿæ”¹ä¸ºå¼‚æ­¥ ---
    async def _fetch_pubmed_abstracts_async(
        self,
        query: str,
        num_results: int = 5,
        max_page: int = 10,
        run_manager: Optional[AsyncCallbackManagerForToolRun] = None
    ) -> List[PubMedResult]:
        logger.debug(f"[è°ƒè¯•] _fetch_pubmed_abstracts_async: query={query}, num_results={num_results}")
        encoded_query = quote_plus(query)
        results = []
        page = 1
        base_url = "https://pubmed.ncbi.nlm.nih.gov/"

        # --- > æ³¨æ„ï¼šhttpx.AsyncClient çš„ headers æ˜¯åœ¨åˆ›å»ºæ—¶è®¾ç½®çš„ < ---
        async with httpx.AsyncClient(headers=BROWSER_HEADERS, timeout=20.0, follow_redirects=True) as client:
             while len(results) < num_results and page <= max_page:
                if run_manager:
                    await run_manager.on_text(f"æ­£åœ¨æœç´¢PubMedç¬¬{page}é¡µ...\n", verbose=True)

                search_url = f"{base_url}?term={encoded_query}&page={page}"
                current_headers = BROWSER_HEADERS.copy() # å¤åˆ¶åŸºç¡€ headers
                # --- > ä¸ºæœç´¢é¡µè®¾ç½® Referer (å¯é€‰) < ---
                if page > 1:
                    current_headers['Referer'] = f"{base_url}?term={encoded_query}&page={page-1}"
                # --- > è®¾ç½®ç»“æŸ < ---

                try:
                    # --- > ä¼ é€’æœ¬æ¬¡è¯·æ±‚çš„ headers (å¦‚æœéœ€è¦åŠ¨æ€ä¿®æ”¹çš„è¯) < ---
                    response = await client.get(search_url, headers=current_headers)
                    response.raise_for_status()
                    data = response.text

                    pat1_content_url = '<div class="docsum-wrap">.*?<.*?href="(.*?)".*?</a>'
                    content_url = re.compile(pat1_content_url, re.S).findall(data)

                    if not content_url: break

                    # --- > åœ¨å¤„ç†æ–‡ç« å‰ç¨ä½œå»¶æ—¶ < ---
                    await asyncio.sleep(0.5) # æš‚åœ 0.5 ç§’
                    # --- > å»¶æ—¶ç»“æŸ < ---

                    for idx, article_url_path in enumerate(content_url):
                        if len(results) >= num_results: break
                        full_url = base_url.rstrip('/') + article_url_path

                        # --- > ä¸ºæ–‡ç« é¡µè®¾ç½® Referer ä¸ºæœç´¢ç»“æœé¡µ < ---
                        article_headers = BROWSER_HEADERS.copy()
                        article_headers['Referer'] = search_url
                        # --- > è®¾ç½®ç»“æŸ < ---

                        if run_manager and idx % 3 == 0:
                            await run_manager.on_text(f"æ­£åœ¨è·å–æ–‡çŒ® {len(results)+1}/{num_results}...\n", verbose=True)

                        try:
                            # --- > è¯·æ±‚é—´çš„å°å»¶æ—¶ < ---
                            if idx > 0: await asyncio.sleep(0.2) # æ¯ç¯‡æ–‡ç« ä¹‹é—´æš‚åœ 0.2 ç§’
                            # --- > å»¶æ—¶ç»“æŸ < ---

                            # --- > ä¼ é€’æœ¬æ¬¡è¯·æ±‚çš„ headers < ---
                            article_response = await client.get(full_url, headers=article_headers)
                            article_response.raise_for_status()
                            article_data = article_response.text

                            # --- > æå– PMID, Title, Abstract < ---
                            pmid_match = re.search(r'/([0-9]+)/?', article_url_path)
                            pmid = pmid_match.group(1) if pmid_match else None
                            if not pmid: continue

                            soup = BeautifulSoup(article_data, 'html.parser')
                            title_tag = soup.select_one('h1.heading-title')
                            title = title_tag.get_text(strip=True) if title_tag else "æ ‡é¢˜æœªæ‰¾åˆ°"

                            abstract_div = soup.select_one('.abstract-content.selected')
                            if not abstract_div:
                                abstract_div = soup.select_one('.abstract-content')

                            abstract = abstract_div.get_text(strip=True) if abstract_div else "æ‘˜è¦æœªæ‰¾åˆ°"
                            abstract_found = bool(abstract and abstract != "æ‘˜è¦æœªæ‰¾åˆ°")
                            # --- > æå–ç»“æŸ < ---

                            if abstract_found:
                                results.append(PubMedResult(pmid=pmid, title=title, abstract=abstract, url=full_url))
                                if run_manager:
                                    await run_manager.on_text(f"âœ“ å·²è·å–: {title[:40]}...\n", verbose=True)

                        except httpx.HTTPStatusError as http_err:
                             logger.error(f"è·å–æ–‡ç« è¯¦æƒ… HTTP é”™è¯¯ {http_err.response.status_code} for {full_url}: {http_err}")
                             if run_manager: await run_manager.on_text(f"è·å–æ–‡ç«  {pmid} è¯¦æƒ…å¤±è´¥ (HTTP {http_err.response.status_code})ï¼Œç»§ç»­\n", verbose=True)
                        except Exception as err:
                            logger.error(f"å¤„ç†PubMedæ–‡ç«  {full_url} æ—¶å‡ºç°é”™è¯¯: {err}", exc_info=True)
                            if run_manager: await run_manager.on_text(f"è·å–æ–‡ç« è¯¦æƒ…å¤±è´¥ï¼Œç»§ç»­å°è¯•ä¸‹ä¸€ç¯‡\n", verbose=True)

                    page += 1
                    # --- > ç¿»é¡µå‰ç¨ä½œå»¶æ—¶ < ---
                    await asyncio.sleep(1.0) # æš‚åœ 1 ç§’
                    # --- > å»¶æ—¶ç»“æŸ < ---

                except httpx.RequestError as req_err:
                    logger.error(f"è¯·æ±‚PubMedç¬¬{page}é¡µæ—¶å‡ºé”™: {req_err}")
                    if run_manager: await run_manager.on_text(f"æœç´¢PubMedç¬¬{page}é¡µè¯·æ±‚å‡ºé”™ï¼Œå°è¯•ä¸‹ä¸€é¡µ\n", verbose=True)
                    page += 1
                    await asyncio.sleep(2.0) # å‡ºé”™åç­‰å¾…æ—¶é—´é•¿ä¸€ç‚¹
                except Exception as err:
                    logger.error(f"å¤„ç†PubMedç¬¬{page}é¡µæ—¶å‡ºç°æœªçŸ¥é”™è¯¯: {err}", exc_info=True)
                    if run_manager: await run_manager.on_text(f"æœç´¢PubMedç¬¬{page}é¡µå‡ºé”™ï¼Œå°è¯•ä¸‹ä¸€é¡µ\n", verbose=True)
                    page += 1
                    await asyncio.sleep(2.0) # å‡ºé”™åç­‰å¾…æ—¶é—´é•¿ä¸€ç‚¹
        return results


# åˆ›å»ºå·¥å…·å®ä¾‹
FastAbstractTool = create_logged_tool(FastAbstractTool)
fast_abstract_tool = FastAbstractTool()
